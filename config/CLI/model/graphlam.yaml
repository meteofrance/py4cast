model:
  model_name: GraphLam
  loss_name: mse # mse or mae
  num_inter_steps: 1 # Number of intermediary steps (without any data)
  num_samples_to_plot: 1
  training_strategy: diff_ar # diff_ar or scaled_ar
  channels_last: False # True: B W H C
  io_conf : null
  mask_ratio : 0 # 0<mask_ratio<1. If !=0 apply maskedautoencoderstrategy.
  learning_rate: 1e-3
  min_learning_rate: 3e-7
  num_warmup_steps: 1000
  betas: [0.9, 0.95]
  settings_init_args:
    tmp_dir: /tmp  # nosec B108
    hidden_dims: 64
    hidden_layers: 1
    use_checkpointing: False
    offload_to_cpu: False
    mesh_aggr: sum
    processor_layers: 4