model:
  model_name: UNetRPP
  loss_name: mse # mse or mae
  num_inter_steps: 1 # Number of intermediary steps (without any data)
  num_samples_to_plot: 1
  training_strategy: scaled_ar # diff_ar or scaled_ar or downscaling_only
  channels_last: False # True: B W H C
  io_conf : config/IO/titan_grib_settings.json
  mask_ratio : 0 # 0<mask_ratio<1. If !=0 apply maskedautoencoderstrategy.
  learning_rate: 1e-3
  min_learning_rate: 3e-7
  num_warmup_steps: 1000
  betas: [0.9, 0.95]
  settings_init_args:
    hidden_size: 1024
    num_heads_encoder: 16
    num_heads_decoder: 4
    pos_embed: "perceptron"
    norm_name: "instance"
    dropout_rate: 0.0
    depths: [3, 3, 3, 3]
    conv_op: "Conv2d"
    # do_ds: false # not recognized by CLI but present in UNetRPPSettings -> strange O.o
    # spatial_dims: 2 # not recognized by CLI but present in UNetRPPSettings -> strange O.o
    linear_upsampling: true
    downsampling_rate: 4
    decoder_proj_size: 64
    encoder_proj_sizes: [64, 64, 64, 32]
    add_skip_connections: true
    attention_code: "torch"